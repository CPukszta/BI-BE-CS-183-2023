{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CPukszta/BI-BE-CS-183-2023/blob/main/HW10/HW10Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MozW3XxY8ezr"
      },
      "source": [
        "Bi/Be/Cs 183 2022-2023: Intro to Computational Biology\n",
        "TAs: Meichen Fang, Tara Chari, Zitong (Jerry) Wang\n",
        "\n",
        "**Submit your notebooks by sharing a clickable link with Viewer access. Link must be accessible from submitted assignment document.**\n",
        "\n",
        "Make sure Runtime $\\rightarrow$ Restart and run all works without error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mu4obGyqS8fh"
      },
      "source": [
        "**HW 10 Final Problem**\n",
        "\n",
        "In this problem you will process a single-cell dataset from the raw fastqs (sequencing reads of the cDNA library) to produce the cell x gene count matrix we usually work with. Given this count matrix you will additionally investigate the impact of various normalization techniques and dimensionality reduction on clustering of the cells (i.e. looking for cell types). With the metadata for this dataset, the cell types and ages of the mice used, you will then compare the efficacy of logistic regression vs neural network based techniques for classifying the age of the mouse a cell came from.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJ8J92ntz2rk"
      },
      "source": [
        "##**Install packages**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5QduXCj1tLC"
      },
      "source": [
        "Install kb-python\n",
        "\n",
        "This package is used to do transcript quantification (as shown in HW 4 Problem 2), aligning sequencing reads to a provided transcriptome to estimate transcript abundances. This produces the gene count matrices we have been working with. kb takes in the raw FASTQ files (the cDNA sequences from the generated cDNA library), aligns the transcript sequences to a reference file for the organism (the kallisto index below), and generates a count matrix of the transcripts (or genes) per cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwDBoJ3Q1ss8",
        "outputId": "81a4ce32-e684-4d25-e78c-2c1fa5140fc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting kb-python\n",
            "  Downloading kb_python-0.27.3.tar.gz (7.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting anndata>=0.6.22.post1\n",
            "  Downloading anndata-0.8.0-py3-none-any.whl (96 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.1/96.1 KB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: h5py>=2.10.0 in /usr/local/lib/python3.9/dist-packages (from kb-python) (3.1.0)\n",
            "Requirement already satisfied: Jinja2>2.10.1 in /usr/local/lib/python3.9/dist-packages (from kb-python) (3.1.2)\n",
            "Collecting loompy>=3.0.6\n",
            "  Downloading loompy-3.0.7.tar.gz (4.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: nbconvert>=5.6.0 in /usr/local/lib/python3.9/dist-packages (from kb-python) (6.5.4)\n",
            "Requirement already satisfied: nbformat>=4.4.0 in /usr/local/lib/python3.9/dist-packages (from kb-python) (5.7.3)\n",
            "Collecting ngs-tools>=1.7.3\n",
            "  Downloading ngs-tools-1.8.3.tar.gz (45.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.9/dist-packages (from kb-python) (1.22.4)\n",
            "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from kb-python) (1.3.5)\n",
            "Requirement already satisfied: plotly>=4.5.0 in /usr/local/lib/python3.9/dist-packages (from kb-python) (5.5.0)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.9/dist-packages (from kb-python) (2.25.1)\n",
            "Collecting scanpy>=1.4.4.post1\n",
            "  Downloading scanpy-1.9.3-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m76.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.9/dist-packages (from kb-python) (1.2.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.9/dist-packages (from kb-python) (4.5.0)\n",
            "Requirement already satisfied: scipy>1.4 in /usr/local/lib/python3.9/dist-packages (from anndata>=0.6.22.post1->kb-python) (1.10.1)\n",
            "Requirement already satisfied: natsort in /usr/local/lib/python3.9/dist-packages (from anndata>=0.6.22.post1->kb-python) (5.5.0)\n",
            "Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.9/dist-packages (from anndata>=0.6.22.post1->kb-python) (23.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from Jinja2>2.10.1->kb-python) (2.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from loompy>=3.0.6->kb-python) (57.4.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.9/dist-packages (from loompy>=3.0.6->kb-python) (0.56.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from loompy>=3.0.6->kb-python) (8.1.3)\n",
            "Collecting numpy-groupies\n",
            "  Downloading numpy_groupies-0.9.20-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.9/dist-packages (from nbconvert>=5.6.0->kb-python) (0.7.1)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.9/dist-packages (from nbconvert>=5.6.0->kb-python) (1.5.0)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.9/dist-packages (from nbconvert>=5.6.0->kb-python) (0.2.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.9/dist-packages (from nbconvert>=5.6.0->kb-python) (4.9.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.9/dist-packages (from nbconvert>=5.6.0->kb-python) (4.6.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.9/dist-packages (from nbconvert>=5.6.0->kb-python) (6.0.0)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.9/dist-packages (from nbconvert>=5.6.0->kb-python) (0.7.2)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.9/dist-packages (from nbconvert>=5.6.0->kb-python) (0.4)\n",
            "Requirement already satisfied: pygments>=2.4.1 in /usr/local/lib/python3.9/dist-packages (from nbconvert>=5.6.0->kb-python) (2.6.1)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.9/dist-packages (from nbconvert>=5.6.0->kb-python) (1.2.1)\n",
            "Requirement already satisfied: jupyter-core>=4.7 in /usr/local/lib/python3.9/dist-packages (from nbconvert>=5.6.0->kb-python) (5.2.0)\n",
            "Requirement already satisfied: traitlets>=5.0 in /usr/local/lib/python3.9/dist-packages (from nbconvert>=5.6.0->kb-python) (5.7.1)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.9/dist-packages (from nbconvert>=5.6.0->kb-python) (0.8.4)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.9/dist-packages (from nbformat>=4.4.0->kb-python) (2.16.3)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.9/dist-packages (from nbformat>=4.4.0->kb-python) (4.3.3)\n",
            "Requirement already satisfied: joblib>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from ngs-tools>=1.7.3->kb-python) (1.2.0)\n",
            "Collecting pysam>=0.16.0.1\n",
            "  Downloading pysam-0.20.0-cp39-cp39-manylinux_2_24_x86_64.whl (15.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.6/15.6 MB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting shortuuid>=1.0.1\n",
            "  Downloading shortuuid-1.0.11-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: tqdm>=4.50.0 in /usr/local/lib/python3.9/dist-packages (from ngs-tools>=1.7.3->kb-python) (4.65.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.0.0->kb-python) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.0.0->kb-python) (2022.7.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from plotly>=4.5.0->kb-python) (8.2.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from plotly>=4.5.0->kb-python) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.22.0->kb-python) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.22.0->kb-python) (1.26.14)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests>=2.22.0->kb-python) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.22.0->kb-python) (2022.12.7)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.9/dist-packages (from scanpy>=1.4.4.post1->kb-python) (0.11.2)\n",
            "Requirement already satisfied: statsmodels>=0.10.0rc2 in /usr/local/lib/python3.9/dist-packages (from scanpy>=1.4.4.post1->kb-python) (0.13.5)\n",
            "Collecting session-info\n",
            "  Downloading session_info-1.0.0.tar.gz (24 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: patsy in /usr/local/lib/python3.9/dist-packages (from scanpy>=1.4.4.post1->kb-python) (0.5.3)\n",
            "Collecting umap-learn>=0.3.10\n",
            "  Downloading umap-learn-0.5.3.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.2/88.2 KB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib>=3.4 in /usr/local/lib/python3.9/dist-packages (from scanpy>=1.4.4.post1->kb-python) (3.5.3)\n",
            "Requirement already satisfied: networkx>=2.3 in /usr/local/lib/python3.9/dist-packages (from scanpy>=1.4.4.post1->kb-python) (3.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.21.3->kb-python) (3.1.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema>=2.6->nbformat>=4.4.0->kb-python) (22.2.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema>=2.6->nbformat>=4.4.0->kb-python) (0.19.3)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.9/dist-packages (from jupyter-core>=4.7->nbconvert>=5.6.0->kb-python) (3.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.4->scanpy>=1.4.4.post1->kb-python) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.4->scanpy>=1.4.4.post1->kb-python) (4.39.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.4->scanpy>=1.4.4.post1->kb-python) (1.4.4)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.4->scanpy>=1.4.4.post1->kb-python) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.4->scanpy>=1.4.4.post1->kb-python) (3.0.9)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.9/dist-packages (from nbclient>=0.5.0->nbconvert>=5.6.0->kb-python) (6.1.12)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.9/dist-packages (from numba->loompy>=3.0.6->kb-python) (0.39.1)\n",
            "Collecting pynndescent>=0.5\n",
            "  Downloading pynndescent-0.5.8.tar.gz (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.9/dist-packages (from bleach->nbconvert>=5.6.0->kb-python) (0.5.1)\n",
            "Collecting stdlib_list\n",
            "  Downloading stdlib_list-0.8.0-py3-none-any.whl (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.5/63.5 KB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.9/dist-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert>=5.6.0->kb-python) (23.2.1)\n",
            "Requirement already satisfied: tornado>=4.1 in /usr/local/lib/python3.9/dist-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert>=5.6.0->kb-python) (6.2)\n",
            "Building wheels for collected packages: kb-python, loompy, ngs-tools, umap-learn, session-info, pynndescent\n",
            "  Building wheel for kb-python (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kb-python: filename=kb_python-0.27.3-py3-none-any.whl size=7543276 sha256=a94a31ca51362a0dd7a6557230410ff7ccd3556c37a71c52d6d0c63be2af2539\n",
            "  Stored in directory: /root/.cache/pip/wheels/cd/b6/e0/4ad2d814ada70c6837917c73149ca675ac8adbc8a0d18a046c\n",
            "  Building wheel for loompy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for loompy: filename=loompy-3.0.7-py3-none-any.whl size=52040 sha256=68e4777840c80457199f3fedc726ed7d6e8d2cde2bbcec380b8513b53854512d\n",
            "  Stored in directory: /root/.cache/pip/wheels/d9/b5/43/cab70cefd40f17c3c39ba18d115ee28b76c6a076dee5abda6e\n",
            "  Building wheel for ngs-tools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ngs-tools: filename=ngs_tools-1.8.3-py3-none-any.whl size=45158524 sha256=774f3ac2cfbfa2be1e5d0dcc98e6a68c61a6216b953a7fa0d71bbbd5ed0eac17\n",
            "  Stored in directory: /root/.cache/pip/wheels/ed/54/5c/3803b2f7e7e535980f0c09176d9d3ee91311d7ed942bdf0868\n",
            "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for umap-learn: filename=umap_learn-0.5.3-py3-none-any.whl size=82829 sha256=6ff13a50acf928ecf2de69d502f6c4941322f9c6584c218fe7d311843cbcb15a\n",
            "  Stored in directory: /root/.cache/pip/wheels/f4/3e/1c/596d0a463d17475af648688443fa4846fef624d1390339e7e9\n",
            "  Building wheel for session-info (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for session-info: filename=session_info-1.0.0-py3-none-any.whl size=8046 sha256=074dfdcce8954ba2557da043ea281ef86d228f1c400d6f466bae073e3ba6f083\n",
            "  Stored in directory: /root/.cache/pip/wheels/d4/fc/2e/00ca60bac7954b84907efd41baa9b4853500eaeec4228410c6\n",
            "  Building wheel for pynndescent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pynndescent: filename=pynndescent-0.5.8-py3-none-any.whl size=55513 sha256=19f42b685f70f14200d27aec316afdf1c8a7b5a94819287e8b2808af750a5ab7\n",
            "  Stored in directory: /root/.cache/pip/wheels/b9/89/cc/59ab91ef5b21dc2ab3635528d7d227f49dfc9169905dcb959d\n",
            "Successfully built kb-python loompy ngs-tools umap-learn session-info pynndescent\n",
            "Installing collected packages: stdlib_list, pysam, shortuuid, session-info, numpy-groupies, ngs-tools, loompy, pynndescent, anndata, umap-learn, scanpy, kb-python\n",
            "Successfully installed anndata-0.8.0 kb-python-0.27.3 loompy-3.0.7 ngs-tools-1.8.3 numpy-groupies-0.9.20 pynndescent-0.5.8 pysam-0.20.0 scanpy-1.9.3 session-info-1.0.0 shortuuid-1.0.11 stdlib_list-0.8.0 umap-learn-0.5.3\n"
          ]
        }
      ],
      "source": [
        "# Install kb. This package runs kallisto and bustools. \n",
        "# These are programs used to process the single-cell RNA-seq reads to produce count matrices.\n",
        "!pip3 install kb-python "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IgmwCBzctnDD",
        "outputId": "6f7cd23a-600b-4a6b-8d0e-66819b408243"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.9/69.9 KB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip3 install --quiet anndata\n",
        "!pip install --quiet scanpy==1.7.0rc1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "rjy8W5hly24q"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import scipy.io as sio\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt #Can use other plotting packages like seaborn\n",
        "\n",
        "import bokeh.io\n",
        "import bokeh.plotting\n",
        "\n",
        "bokeh.io.output_notebook()\n",
        "\n",
        "import anndata\n",
        "import scanpy as sc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0Ep8gQNCNLHr"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "t=time.time()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpeuucqgiDPx"
      },
      "source": [
        "## **Read in data for Part a) analysis**\n",
        "#### Running the code for data downloading and count matrix generation may take ~20mins total."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUWXJ6GJ2ZbY",
        "outputId": "92f1dfc9-3ff5-41da-d78c-b1cacd33e648"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-12 23:16:13--  ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR842/002/SRR8426372/SRR8426372_1.fastq.gz\n",
            "           => ‘SRR8426372_1.fastq.gz’\n",
            "Resolving ftp.sra.ebi.ac.uk (ftp.sra.ebi.ac.uk)... 193.62.193.138\n",
            "Connecting to ftp.sra.ebi.ac.uk (ftp.sra.ebi.ac.uk)|193.62.193.138|:21... connected.\n",
            "Logging in as anonymous ... Logged in!\n",
            "==> SYST ... done.    ==> PWD ... done.\n",
            "==> TYPE I ... done.  ==> CWD (1) /vol1/fastq/SRR842/002/SRR8426372 ... done.\n",
            "==> SIZE SRR8426372_1.fastq.gz ... 2389120304\n",
            "==> PASV ... done.    ==> RETR SRR8426372_1.fastq.gz ... done.\n",
            "Length: 2389120304 (2.2G) (unauthoritative)\n",
            "\n",
            "SRR8426372_1.fastq. 100%[===================>]   2.22G  36.0MB/s    in 65s     \n",
            "\n",
            "2023-03-12 23:17:19 (35.2 MB/s) - ‘SRR8426372_1.fastq.gz’ saved [2389120304]\n",
            "\n",
            "--2023-03-12 23:17:20--  ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR842/002/SRR8426372/SRR8426372_2.fastq.gz\n",
            "           => ‘SRR8426372_2.fastq.gz’\n",
            "Resolving ftp.sra.ebi.ac.uk (ftp.sra.ebi.ac.uk)... 193.62.193.138\n",
            "Connecting to ftp.sra.ebi.ac.uk (ftp.sra.ebi.ac.uk)|193.62.193.138|:21... connected.\n",
            "Logging in as anonymous ... Logged in!\n",
            "==> SYST ... done.    ==> PWD ... done.\n",
            "==> TYPE I ... done.  ==> CWD (1) /vol1/fastq/SRR842/002/SRR8426372 ... done.\n",
            "==> SIZE SRR8426372_2.fastq.gz ... 3208503807\n",
            "==> PASV ... done.    ==> RETR SRR8426372_2.fastq.gz ... done.\n",
            "Length: 3208503807 (3.0G) (unauthoritative)\n",
            "\n",
            "SRR8426372_2.fastq. 100%[===================>]   2.99G  36.1MB/s    in 88s     \n",
            "\n",
            "2023-03-12 23:18:49 (34.9 MB/s) - ‘SRR8426372_2.fastq.gz’ saved [3208503807]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Download the data from the ENA for a 3-month mouse\n",
        "# This step should take 5-10mins\n",
        "!wget --continue ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR842/002/SRR8426372/SRR8426372_1.fastq.gz\n",
        "!wget --continue ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR842/002/SRR8426372/SRR8426372_2.fastq.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48o3Q_8v2o72"
      },
      "source": [
        "Download a kallisto index \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HDmVMTbs3BSV",
        "outputId": "bc9788c4-52de-47b7-a6df-a2f97bdab26f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2023-03-12 23:18:52,741]    INFO [download] Downloading files for mouse from https://caltech.box.com/shared/static/vcaz6cujop0xuapdmz0pplp3aoqc41si.gz to tmp/vcaz6cujop0xuapdmz0pplp3aoqc41si.gz\n",
            "100% 1.89G/1.89G [02:18<00:00, 14.6MB/s]\n",
            "[2023-03-12 23:21:11,338]    INFO [download] Extracting files from tmp/vcaz6cujop0xuapdmz0pplp3aoqc41si.gz\n"
          ]
        }
      ],
      "source": [
        "!kb ref -d mouse -i index.idx -g t2g.txt -f1 transcriptome.fasta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yoezewx53Os4"
      },
      "source": [
        "Generate the cell x gene count matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtJ_aet53CE8",
        "outputId": "8a2cbfb5-732f-4181-9f75-786543542e4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2023-03-12 23:32:37,676]    INFO [count] Using index index.idx to generate BUS file to output from\n",
            "[2023-03-12 23:32:37,680]    INFO [count]         SRR8426372_1.fastq.gz\n",
            "[2023-03-12 23:32:37,684]    INFO [count]         SRR8426372_2.fastq.gz\n",
            "[2023-03-12 23:43:44,049]    INFO [count] Sorting BUS file output/output.bus to output/tmp/output.s.bus\n",
            "[2023-03-12 23:44:08,487]    INFO [count] Whitelist not provided\n",
            "[2023-03-12 23:44:08,488]    INFO [count] Generating whitelist output/whitelist.txt from BUS file output/tmp/output.s.bus\n",
            "[2023-03-12 23:44:09,599]    INFO [count] Inspecting BUS file output/tmp/output.s.bus\n",
            "[2023-03-12 23:44:11,025]    INFO [count] Correcting BUS records in output/tmp/output.s.bus to output/tmp/output.s.c.bus with whitelist output/whitelist.txt\n",
            "[2023-03-12 23:44:14,753]    INFO [count] Sorting BUS file output/tmp/output.s.c.bus to output/output.unfiltered.bus\n",
            "[2023-03-12 23:44:21,044]    INFO [count] Generating count matrix output/counts_unfiltered/cells_x_genes from BUS file output/output.unfiltered.bus\n",
            "[2023-03-12 23:44:26,255]    INFO [count] Reading matrix output/counts_unfiltered/cells_x_genes.mtx\n",
            "[2023-03-12 23:44:27,822]    INFO [count] Writing matrix to h5ad output/counts_unfiltered/adata.h5ad\n",
            "[2023-03-12 23:44:27,967]    INFO [count] Filtering with bustools\n",
            "[2023-03-12 23:44:27,967]    INFO [count] Generating whitelist output/filter_barcodes.txt from BUS file output/output.unfiltered.bus\n",
            "[2023-03-12 23:44:29,106]    INFO [count] Correcting BUS records in output/output.unfiltered.bus to output/tmp/output.unfiltered.c.bus with whitelist output/filter_barcodes.txt\n",
            "[2023-03-12 23:44:31,680]    INFO [count] Sorting BUS file output/tmp/output.unfiltered.c.bus to output/output.filtered.bus\n",
            "[2023-03-12 23:44:39,690]    INFO [count] Generating count matrix output/counts_filtered/cells_x_genes from BUS file output/output.filtered.bus\n",
            "[2023-03-12 23:44:49,081]    INFO [count] Reading matrix output/counts_filtered/cells_x_genes.mtx\n",
            "[2023-03-12 23:44:50,084]    INFO [count] Writing matrix to h5ad output/counts_filtered/adata.h5ad\n",
            "CPU times: user 5.49 s, sys: 773 ms, total: 6.26 s\n",
            "Wall time: 12min 19s\n"
          ]
        }
      ],
      "source": [
        "# This command processes previously downloaded data\n",
        "# This step should take ~11mins\n",
        "%%time\n",
        "!kb count --h5ad -i index.idx -g t2g.txt -x Dropseq -o output --filter bustools -t 2 \\\n",
        "SRR8426372_1.fastq.gz \\\n",
        "SRR8426372_2.fastq.gz\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swY2nSDukmXg"
      },
      "source": [
        "**The dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKSN3bu8cQ3O"
      },
      "source": [
        "This is a Drop-seq based single-cell RNA-seq dataset produced from tissue extracted from the whole mouse lung, published by [Ilias Angelidis, Lukas M. Simon et al. 2019](https://www.nature.com/articles/s41467-019-08831-9). In the study single-cell suspensions were generated from eight 3-month old mice and seven 24-month old mice and looked for cell type specific effects of aging between the mice i.e. to create a single-cell atlas of the aging lung. \n",
        "\n",
        "For Part a we will only be working with one sample from a 3-month old mouse (though in Parts b-d you will work with the full dataset across both ages and all mice)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1E4wbXtSsJN"
      },
      "source": [
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1O_x3hmDDes7foQVVLcoZVasrVED1L0Al\" alt=\"EMFigure\" width=\"900\" height=\"150\"><center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58nQkH9eko4-"
      },
      "source": [
        "**The count matrix**\n",
        "\n",
        "This matrix is 3,839 cells by 55,421 genes for one lung sample from a 3-month old mouse.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "3_s_MyJp3dj9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 524
        },
        "outputId": "15ad57be-5ce4-4935-fa65-5acd48c10e82"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-0de1e91996f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# load the raw cell x gene count matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0madata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manndata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_h5ad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"output/counts_unfiltered/adata.h5ad\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#This is the output from kb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0madata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"gene_id\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mt2g\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"t2g.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tid\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"gene_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"gene_name\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Load the transcipt-to-gene name mapping (t2g)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/anndata/_io/h5ad.py\u001b[0m in \u001b[0;36mread_h5ad\u001b[0;34m(filename, backed, as_sparse, as_sparse_fmt, chunk_size)\u001b[0m\n\u001b[1;32m    222\u001b[0m     )\n\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, **kwds)\u001b[0m\n\u001b[1;32m    422\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mphil\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m                 \u001b[0mfapl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fapl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrdcc_nslots\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrdcc_nbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrdcc_w0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m                 fid = make_fid(name, mode, userblock_size,\n\u001b[0m\u001b[1;32m    425\u001b[0m                                fapl, fcpl=make_fcpl(track_order=track_order, fs_strategy=fs_strategy,\n\u001b[1;32m    426\u001b[0m                                fs_persist=fs_persist, fs_threshold=fs_threshold),\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: Unable to open file (unable to open file: name = 'output/counts_unfiltered/adata.h5ad', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
          ]
        }
      ],
      "source": [
        "# load the raw cell x gene count matrix\n",
        "adata = anndata.read_h5ad(\"output/counts_unfiltered/adata.h5ad\") #This is the output from kb\n",
        "adata.var[\"gene_id\"] = adata.var.index.values\n",
        "\n",
        "t2g = pd.read_csv(\"t2g.txt\", header=None, names=[\"tid\", \"gene_id\", \"gene_name\"], sep=\"\\t\") #Load the transcipt-to-gene name mapping (t2g)\n",
        "t2g.index = t2g.gene_id\n",
        "t2g = t2g.loc[~t2g.index.duplicated(keep='first')]\n",
        "\n",
        "adata.var[\"gene_name\"] = adata.var.gene_id.map(t2g[\"gene_name\"])\n",
        "adata.var.index = adata.var[\"gene_name\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5tLkEyK3z0GX"
      },
      "outputs": [],
      "source": [
        "adata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tegjqi-7rsoq"
      },
      "outputs": [],
      "source": [
        "count_mat = adata.X #Get the count matrix from this anndata object\n",
        "count_mat.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beHUeEQ0PYnj"
      },
      "source": [
        "**Use this count_mat for Part a.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICnf8oOmNTOy"
      },
      "source": [
        "# **a) Pre-processing: Select real/valid cells i.e. cells that pass a UMI count threshold based on the commonly used 'Knee plot'. (10 points)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTIoUbKGN5UD"
      },
      "source": [
        "Knee plots (described below) are commonly used to filter out cell barcodes that likely correspond to empty droplets that were captured or noisy samples that may be just random transcripts that were picked up in the droplet. We want to only keep cell barcodes that seem to have high enough UMI counts (i.e. molecules detected) which suggest that a real cell was captured in that droplet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufFe0UnuOZt9"
      },
      "source": [
        "**To construct a knee plot** we (1) rank cells in *descending* order of their total UMI counts (UMI counts summed across genes). The cell rankings are plotted on the x axis (1 to n cells). On the y-axis we (2) plot the total UMI count of each cell. Thus as we move across the x-axis from left to right, the right end of the plot displays cells with very few UMI counts (noisy/empty droplets to possibly remove from analysis). Often the x and y axis are plotted in a log-log plot."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQgUATd4O2dT"
      },
      "source": [
        "The inflection point of the graph denotes a separation/drop between the lefthand side of the plot where cell barcodes have high UMI counts and the righthand side where cell barcodes have low associated UMI counts (and are thus considered to have had failure in capture and/or to be too noisy for further analysis.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffP0IA4wQKYQ"
      },
      "source": [
        "**(1) Make one knee plot for the 3,839 cells (which are the cell barcodes) and their total UMI counts across the 55,421 genes (in a log-log plot). (2) Describe what UMI threshold you might use to filter out noisy cell barcodes based on the plot.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjOR_hLaPqyx"
      },
      "outputs": [],
      "source": [
        "#lets start by rearranging the count matrix in order of highest to lowest UMI counts \n",
        "Count_mat_sorted = count_mat[np.argsort(count_mat.sum(axis=1))[::-1],:]\n",
        "UMI_sum_Sorted = np.sort(count_mat.sum(axis=1))[::-1]\n",
        "\n",
        "knee = bokeh.plotting.figure(width = 400, height = 450,\n",
        "                          title = \"Knee Plot\",\n",
        "                          x_axis_label = \"Cell Number\", y_axis_label=\"UMI Sum\")\n",
        "\n",
        "knee.circle(np.linspace((1,np.shape(count_mat)[0])),UMI_sum_Sorted)\n",
        "\n",
        "bokeh.io.show(knee)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It looks like we should keep the top cells of the cells sorted by UMI count. "
      ],
      "metadata": {
        "id": "SDd7HEhJKAFc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mKX2DOqkTnWh"
      },
      "outputs": [],
      "source": [
        "cut_off = 8\n",
        "\n",
        "filt_count_mat = Count_mat_sorted[0:cut_off,:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQDAIuPCQze5"
      },
      "source": [
        "# **Read in data for Parts b-d analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGPCUCKaTt0R"
      },
      "source": [
        "For Parts b-d you will be using the full gene count matrix across the 3-month and 24-month mouse cells combined, as provided in the original paper, which is 14,813 cells × 21,969 genes. We will filter for the top 2000 highly variable genes, so that you can use this matrix within the Colab environment. (Downloaded below)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "luQCozWsMUVI"
      },
      "outputs": [],
      "source": [
        "#Read in full cell x gene count matrix for 3 and 24 month old mice lung samples\n",
        "!wget --content-disposition https://ftp.ncbi.nlm.nih.gov/geo/series/GSE124nnn/GSE124872/suppl/GSE124872_raw_counts_single_cell.mtx.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQsw10rIMUVI"
      },
      "outputs": [],
      "source": [
        "#Read in cell metadata\n",
        "!wget --content-disposition https://ftp.ncbi.nlm.nih.gov/geo/series/GSE124nnn/GSE124872/suppl/GSE124872_Angelidis_2018_metadata.csv.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0H4GbBIMUVJ"
      },
      "outputs": [],
      "source": [
        "!gunzip GSE124872_Angelidis_2018_metadata.csv.gz\n",
        "!gunzip GSE124872_raw_counts_single_cell.mtx.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XidblsX0BcX"
      },
      "source": [
        "Read in metadata of cells: \"grouping\" denotes the age (3m - 3 month, 24m - 24 month) and \"celltype\" denotes the cell type e.g. Plasma_cells"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00wWx2nxqZat"
      },
      "outputs": [],
      "source": [
        "meta = pd.read_csv('GSE124872_Angelidis_2018_metadata.csv')\n",
        "meta.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1yZZZYuMiEO"
      },
      "source": [
        "**The dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZ59i_izMiEO"
      },
      "source": [
        "This is a Drop-seq based single-cell RNA-seq dataset produced from tissue extracted from the whole mouse lung, published by [Ilias Angelidis, Lukas M. Simon et al. 2019](https://www.nature.com/articles/s41467-019-08831-9). In the study single-cell suspensions were generated from eight 3-month old mice and seven 24-month old mice and looked for cell type specific effects of aging between the mice i.e. to create a single-cell atlas of the aging lung. \n",
        "\n",
        "For Parts b-c we will only be working with the full dataset across both ages and all mice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGSuxY1SMiEP"
      },
      "source": [
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1O_x3hmDDes7foQVVLcoZVasrVED1L0Al\" alt=\"EMFigure\" width=\"900\" height=\"150\"><center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAzghRuSMiEP"
      },
      "source": [
        "**The count matrix**\n",
        "\n",
        "This matrix is 14,813 cells by 21,969 genes across all mouse lung samples as provided in Ilias Angelidis, Lukas M. Simon et al. 2019.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SGWfbirV8ne"
      },
      "source": [
        "Read in count matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h3NQrR1zWFqp"
      },
      "outputs": [],
      "source": [
        "import scipy.io as sio\n",
        "count_mat = sio.mmread('GSE124872_raw_counts_single_cell.mtx')\n",
        "count_mat = count_mat.todense().T\n",
        "count_mat.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2iimKfnAgxYo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_De9ShHWJo-"
      },
      "source": [
        "Select for top 2000 highly variable genes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dpOyRjceWMOd"
      },
      "outputs": [],
      "source": [
        "#Anndata is a common data type for processing and storing single-cell count matrices\n",
        "adata = anndata.AnnData(X = count_mat)\n",
        "adata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uxZCUq45i_NJ"
      },
      "outputs": [],
      "source": [
        "#Select the 'highly variable' genes in the matrix so we don't use 21k genes\n",
        "sc.pp.filter_cells(adata, min_counts=0)\n",
        "sc.pp.filter_genes(adata, min_counts=0)\n",
        "\n",
        "sc.pp.normalize_per_cell(adata, counts_per_cell_after=1e4) #Cell-size normalization\n",
        "sc.pp.log1p(adata) #log-variance stabilization\n",
        "\n",
        "sc.pp.highly_variable_genes(adata,n_top_genes=2000)\n",
        "\n",
        "genesToKeep = adata.var['highly_variable']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SkQSSLeoWMUm"
      },
      "outputs": [],
      "source": [
        "#Subset original count matrix\n",
        "count_mat = count_mat[:,genesToKeep]\n",
        "count_mat.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fq77aUyOlNds"
      },
      "source": [
        "**Use this 14813 × 2000 count_mat matrix as your count matrix for parts b-d**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5cWWScjWVUP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxubBCf6NROW"
      },
      "source": [
        "# **b) Normalization (10 points)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9M9Gi7mki9bF"
      },
      "source": [
        "Note that we use cell size normalization and then log1p to transform data before selecting highly variable genes, and then we use the indices to subset the raw count matrix. Therefore, the count_mat matrix here is the raw count matrix. \n",
        "\n",
        "In this part, you will do size-normalization and log1p transformations yourself. For size-normalization, normalize the data matrix to 10,000 reads per cell.\n",
        "\n",
        "**Report both the raw count matrix and the normalized matrix (that is size-normalized and then log1p transformed).**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw = count_mat\n",
        "\n",
        "raw_df = pd.DataFrame(raw_df)\n",
        "print(raw_df)"
      ],
      "metadata": {
        "id": "D7Mudn5AQ647"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "norm = 10,000\n",
        "cell_totals = np.sum(count_mat,axis=1)\n",
        "normalize = count_mat/(cell_totals) * norm\n",
        "normalize = np.log1p(normalize)\n",
        "\n",
        "normalize_df = pd.DataFrame(normalize)\n",
        "print(raw_df)"
      ],
      "metadata": {
        "id": "7mBQgJlyQ9ry",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 135
        },
        "outputId": "83d0038a-7422-43a7-b2db-df2e637b60ee"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-11-9da832f8daf8>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    normalize = count_mat/(cell_totals) *\u001b[0m\n\u001b[0m                                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fef1K_WBMmQ4"
      },
      "source": [
        "# **c) Clustering (20 points)**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this part, we will do K-means clustering on 4 different count matrices:\n",
        "1. Raw count matix\n",
        "2. Raw count matrix transformed by the first 15 principal components\n",
        "3. Normalized count matix\n",
        "4. Normalized count matix transformed by the first 15 principal components"
      ],
      "metadata": {
        "id": "rHv-fvOcVcD9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You will use sklearn for PCA and [KMeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans.fit_predict) clustering. **Set the cluster number k to 10 and random_state to 0 when using KMeans.**\n",
        "\n",
        "Below is an example from sklearn"
      ],
      "metadata": {
        "id": "nkSjyzITVzTf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "X = np.array([[1, 2], [1, 4], [1, 0],\n",
        "               [10, 2], [10, 4], [10, \n",
        "                                  0]])\n",
        "kmeans = KMeans(n_clusters=2, random_state=0).fit(X)\n",
        "kmeans.labels_"
      ],
      "metadata": {
        "id": "SVDZrPjVVjH1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "components = np.shape(count_mat)[1]\n",
        "pca = PCA(n_components = components, svd_solver='full')\n",
        "pca.fit(count_mat)"
      ],
      "metadata": {
        "id": "FLWhBW5TOD6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#set random state and cluster #\n",
        "rs = 0\n",
        "k= 10\n",
        "\n",
        "kmeans_raw = KMeans(n_clusters=k, random_state=rs).fit(raw)\n",
        "kmeans_rawpc = KMeans(n_clusters=k, random_state=rs).fit(raw_pca)\n",
        "kmeans_norm = KMeans(n_clusters=k, random_state=rs).fit(normalize)\n",
        "kmeans_normpcKMeans = KMeans(n_clusters=k, random_state=rs).fit(normalize_pca)"
      ],
      "metadata": {
        "id": "SDRztiAWNNP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**For each of the four clustering results, compare the k-means cluster assignments to the given cell types in meta data. Report the majority, 'true' cell type in each k-means cluster, and the proportion of cells that are that majority in each k-means cluster.**\n",
        "\n",
        "**For the two clusterings done in 15 dimensional PCA space, also plot the transformed data in 2D (only the first 2 transformed coordinates) colored by corresponding k-means clusters (which was done in the 15D PCA-space). Comment on the differences between the two plots.**"
      ],
      "metadata": {
        "id": "2WLLcEtmhyf0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4luzBqqrMElW"
      },
      "source": [
        "# **d) Age prediction (60 points)**\n",
        "\n",
        "In their work, [Ilias Angelidis, Lukas M. Simon et al.](https://www.nature.com/articles/s41467-019-08831-9) found that the gene expression pattern for some cell types were different between the two age groups (3 months and 24 months), as shown in the figure below. In the figure below, the x-axis represent different genes and the y-axis represent different cell types, with each color bar representing the logarithm of the fold change in expression level between old and young cells for a particular genes in a particular cell type. Inspired by this result, we will build classification models (using normalized counts) and test how well they can predict the age of a mouse based on the gene expression profile of a single cell.\n",
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=17gt0rkWzJmh11GPw6ipehM6miiGTD86e\" alt=\"EMFigure\" width=\"550\" height=\"500\"><center>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will compare the predictive power of two different classes of models: (1) logistic regression and (2) neural network. For each class, we will fit a model using gene expression data from a single cell type to predict the age (3m or 24m) of the mouse from which the cell was taken."
      ],
      "metadata": {
        "id": "jZVB11fIt-3P"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dqq9VguaP-J"
      },
      "source": [
        "**i) From the given information, what cell type would you choose to build a predictive model of mouse age? Explain your reasoning (10 points)**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Your Answer Here:** \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9qHuBsWy4xhb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To standardize the workflow, we will consider just two cell types: \n",
        "\n",
        "(1) 'Alveolar_macrophage' (AM)\n",
        "\n",
        "(2) 'Type_2_pneumocytes' (T2P) \n",
        "\n",
        "For the rest of the question, you will build a total of **4** models: \n",
        "\n",
        "1.   logistic regression using alveolar macrophage \n",
        "2.   logistic regression using type II pneumocytes \n",
        "3.   neural network using alveolar macrophage\n",
        "4.   neural network using type II pneumocytes"
      ],
      "metadata": {
        "id": "p42bDqN0uCvA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We first subset our **normalized count matrix** to get out count data for alveolar macrophages (AM) and type II pneumocytes (T2P) sampled from 3 months and 24 months mouse, as well as the corresponding age/label for each cell. AM_3m represents alveolar macrophages from a 3 months old mouse, T2P_24m represents type II pneumocytes from a 24 months old mouse. *Note that we are representing the age class '3m' using the integer 0 and the age class '24m' using the integer 1.*"
      ],
      "metadata": {
        "id": "-kY83NdUuGMg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "typename = 'Alveolar_macrophage'\n",
        "youngIndex = meta.grouping.isin(['3m'])*meta.celltype.isin([typename])\n",
        "oldIndex = meta.grouping.isin(['24m'])*meta.celltype.isin([typename])\n",
        "\n",
        "# we will assign the number 0 to the age class '3m' and 1 to the age class '24m'\n",
        "AM_3m = normalize[youngIndex,:]\n",
        "AM_3m_label = np.zeros(np.size(AM_3m,0))\n",
        "AM_24m = normalize[oldIndex,:]\n",
        "AM_24m_label = np.ones(np.size(AM_24m,0))\n",
        "\n",
        "typename = 'Type_2_pneumocytes'\n",
        "youngIndex = meta.grouping.isin(['3m'])*meta.celltype.isin([typename])\n",
        "oldIndex = meta.grouping.isin(['24m'])*meta.celltype.isin([typename])\n",
        "\n",
        "T2P_3m = normalize[youngIndex,:]\n",
        "T2P_3m_label = np.zeros(np.size(T2P_3m,0))\n",
        "T2P_24m = normalize[oldIndex,:]\n",
        "T2P_24m_label = np.ones(np.size(T2P_24m,0))"
      ],
      "metadata": {
        "id": "2174-TmsuIyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we will split our data evenly into a training set and test set. To allow for a fair comparison between the two cell types, we will use the same number of cells for alveolar macrophage as type II pneumocytes. **Use the code block below to split the data for each cell type into a train set and a test set, make sure you split both the gene expression counts and the corresponding labels.** For both cell types, the training set will consist of the first 550 cells of the 3 month old mouse and the first 250 cells of the 24 month old, the test set will consist of the subsequent 550 cells of the 3 month old mouse and the subsequent 250 cells of the 24 month old mouse."
      ],
      "metadata": {
        "id": "tWvojPpluJ9c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#properly subset the data and labels into these variables\n",
        "AM_train = # ENTER CODE HERE\n",
        "AM_trainlabel = # ENTER CODE HERE\n",
        "AM_test = # ENTER CODE HERE\n",
        "AM_testlabel = # ENTER CODE HERE\n",
        "\n",
        "T2P_train = # ENTER CODE HERE\n",
        "T2P_trainlabel = # ENTER CODE HERE\n",
        "T2P_test = # ENTER CODE HERE\n",
        "T2P_testlabel = # ENTER CODE HERE"
      ],
      "metadata": {
        "id": "HHLcYdstuNR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22fr_rZvMJJA"
      },
      "source": [
        " **ii) Logistic regression models (10 points)**\n",
        "\n",
        " In this section, you will build a logistic regression model to classify mouse age and use it to predict the age of the mouse from the gene expression profile of a cell. Recall that with logistic regression we model a categorical variable (e.g. mouse age - 3m vs 24m) as a continuous value (i.e. the probability of being in the category). In our case, the independent variable will be the collection of count data across all 2000 genes and the dependent variable will be a binary variable, 0 or 1, representing whether the cell was from the 24 month old mouse."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**For each cell type (AM and T2P), complete the code block below where you will need to:**\n",
        "\n",
        "1) fit a logistic regression model using the training set you created above (hint: you may need to change certain hyperparameter to get convergence)\n",
        "\n",
        "3) evaluating your logistic regression model on the corresponding test set (make sure to use the same cell type as you used to fit the model)\n",
        "\n",
        "4) report the test accuracy of your model as the percent of cells where the age of the mouse was correctly predicted by the model"
      ],
      "metadata": {
        "id": "3MSYvT0yuTjQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4JZ-4qWDbsNz"
      },
      "outputs": [],
      "source": [
        "#Set up model for AM \n",
        "# ENTER CODE HERE\n",
        "\n",
        "#Evaluate logistic regression model on the AM test set\n",
        "# ENTER CODE HERE\n",
        "\n",
        "#Report test accuracy of AM model\n",
        "# ENTER CODE HERE\n",
        "ncorrect = \n",
        "ntotal = \n",
        "print('Test Accuracy: %2.2f %%' % ((100.0 * ncorrect) / ntotal))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Set up model for T2P\n",
        "# ENTER CODE HERE\n",
        "\n",
        "#Evaluate logistic regression model on the T2P test set\n",
        "# ENTER CODE HERE\n",
        "\n",
        "#Report test accuracy of T2P model\n",
        "# ENTER CODE HERE\n",
        "ncorrect = \n",
        "ntotal =\n",
        "print('Test Accuracy: %2.2f %%' % ((100.0 * ncorrect) / ntotal))"
      ],
      "metadata": {
        "id": "EcNP2PfguWZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TY6RN5qlMQ2B"
      },
      "source": [
        "**iii) Neural network models - use the code/function below (30 points)**\n",
        "\n",
        "In this section, we will try to predict mouse age by building a Multi-Layer Perceptron (MLP) Model using [Pytorch](https://pytorch.org/docs/stable/index.html). Our MLP will take as input the expression level of the 2000 highly variable genes for a given cell and produce a pair of values corresponding to the probability of the cell being obtain from a 3 months vs. 24 months mouse. Specifically, we will train two separate neural networks using different cell types, one using **alveolar macrophages** and another using **type II pneumocytes**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8X4-8_RWzkg"
      },
      "source": [
        "We will start by first importing packages and initialize the random number generator to a fixed constant to ensure reproducibility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7YMofxdbq_o"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "seed = 183\n",
        "torch.manual_seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### We will first construct our **neural network model for alveolar macrophages**"
      ],
      "metadata": {
        "id": "F__TdIbQ7jnx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vnkRThppxDC"
      },
      "source": [
        "When we train and evaluate our model, we will require batches of data to be provided. The Dataloader class can automatically provide batches of data fetched from a `Dataset` objects (which is defined for you as a `CountDataset` class). **Complete the code block below by converting counts and labels into tensors of the appropriate type.** In order for training to run properly, you need to make sure to convert count data to [`tensor` objects](https://pytorch.org/docs/stable/tensors.html#torch.Tensor) of type Float and the labels (age) should also be `tensor` objects of type Integer. **Furthermore, set an appropriate batch size, nbatch.** The batch size represents the number of samples passed through our neural network before an error gradient is computed and the network parameters are updated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WUUnh30ZH7P"
      },
      "outputs": [],
      "source": [
        "#create a custom dataset object so we can use dataloader for shuffling the data\n",
        "class CountDataset(Dataset):\n",
        "    def __init__(self, count, labels):\n",
        "        self.labels = labels\n",
        "        self.count = count\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "    def __getitem__(self, idx):\n",
        "        label = self.labels[idx]\n",
        "        count = self.count[idx]\n",
        "        sample = [count, label]\n",
        "        return sample\n",
        "\n",
        "# convert counts/label into tensors of the appropriate type\n",
        "traindata = # ENTER CODE HERE\n",
        "trainlabel = # ENTER CODE HERE\n",
        "testdata = # ENTER CODE HERE\n",
        "testlabel = # ENTER CODE HERE\n",
        "\n",
        "trainset = CountDataset(count=traindata, labels=trainlabel)\n",
        "testset = CountDataset(count=testdata, labels=testlabel)\n",
        "\n",
        "# create data loaders\n",
        "nbatch = # ENTER CODE HERE\n",
        "trainloader = DataLoader(trainset, batch_size=nbatch, shuffle=True)\n",
        "testloader = DataLoader(testset, batch_size=nbatch, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRNdKS0Qkyjs"
      },
      "source": [
        "We are now ready to create our simple neural network model. We will define our model in a class that extends nn.Module. nn.Module subclasses must do a minimum of one thing: implement the forward method which takes a batch of data and performs the forward-pass. PyTorch's autograd system computes the gradients of the forward pass for us. In the code below we'll also make use of the constructor of our model to instantiate the hidden and output layers.The model is a simple neural network with one hidden layer. A rectifier linear unit (ReLU) activation function is used for the neurons in the hidden layer. The nn.Module class defines a instance variable called training that is set to True when the model is being trained and False when it is being evaluated after being trained. Since we would like our output to represent probabilities of the cell being from either of the two age groups (outputs are between 0 and 1, and sum up to 1), we can use the [softmax activation function](https://pytorch.org/docs/stable/generated/torch.nn.functional.softmax.html#torch.nn.functional.softmax) on our output layer to turn the outputs into probability-like values. **Complete the code below by implementing the softmax function on the output when the model is not being trained.** We do this because we will use PyTorch's implementation of Cross Entropy Loss (nn.CrossEntropyLoss) during training which implicitly adds a softmax before a logarithmic loss."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define MLP model\n",
        "class MLPmodel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(MLPmodel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size) \n",
        "        self.fc2 = nn.Linear(hidden_size, num_classes)  \n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = F.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        if not self.training:\n",
        "          # ENTER CODE HERE\n",
        "        return out"
      ],
      "metadata": {
        "id": "oPNaLY6duzaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now fit and evaluate the model. **In the code below, complete the implementation of the training loop by**\n",
        "\n",
        "(1) Initialize an instance of our MLPmodel class using appropriate choices of the input_size, hidden_size, and num_classes.\n",
        "\n",
        "(2) Define the variables named loss_function and optimiser, we will use the [cross entropy loss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss) and the [ADAM optimiser](https://pytorch.org/docs/stable/optim.html).\n",
        "\n",
        "(3) Pick an appropriate number of epoch to train for.\n",
        "\n",
        "(4) Make a line plot showing the loss across all training epochs."
      ],
      "metadata": {
        "id": "iI2hmsDYu1jS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FpXJARJceXM6"
      },
      "outputs": [],
      "source": [
        "# build the model \n",
        "input_size = # ENTER CODE HERE\n",
        "hidden_size = # ENTER CODE HERE\n",
        "num_classes = # ENTER CODE HERE\n",
        "model = MLPmodel(input_size, hidden_size, num_classes)\n",
        "\n",
        "# define the loss function and the optimiser\n",
        "loss_function =# ENTER CODE HERE\n",
        "optimiser = # ENTER CODE HERE\n",
        "\n",
        "# define number of epochs to train for\n",
        "nepoch =# ENTER CODE HERE\n",
        "\n",
        "# the epoch loop\n",
        "for epoch in range(nepoch):\n",
        "    running_loss = 0.0\n",
        "    for data in trainloader:\n",
        "        # get the inputs\n",
        "        inputs, labels = data\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimiser.zero_grad()\n",
        "\n",
        "        # forward + loss + backward + optimise (update weights)\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_function(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimiser.step()\n",
        "\n",
        "        # keep track of the loss this epoch\n",
        "        running_loss += loss.item()\n",
        "\n",
        "# plot epoch vs loss\n",
        "# ENTER CODE HERE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's evaluate the overall accuracy of our trained network on our test set. Use the following code block to finish implementation of the accuracy computation. Report the test accuracy as the percent of cells where the age of the mouse was correctly predicted by the trained MLP model. Note that before the code you need to implement we've made a call to model.eval() - this sets the model into evaluation mode and supresses non-training things (gradients, dropout being applied/computed, etc.)."
      ],
      "metadata": {
        "id": "sw_W4qN8vNT4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "# Compute the model accuracy on the test set\n",
        "# ENTER CODE HERE\n",
        "ncorrect = \n",
        "ntotal = \n",
        "\n",
        "print('Test Accuracy: %2.2f %%' % ((100.0 * ncorrect) / ntotal))"
      ],
      "metadata": {
        "id": "LHgzcmiHvNsP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Repeat the steps above for type II pneumocytes. Use the exact same set of hyperparameters you set for training on alveolar macrophages.**\n",
        "\n",
        "**Show all your code and report the following**:\n",
        "\n",
        "(1) A loss curve showing the loss value for each epoch\n",
        "\n",
        "(2) Test accuracy on the T2P test set"
      ],
      "metadata": {
        "id": "asURYZdRvRu-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train the same MLP model used above for T2P\n",
        "# ENTER CODE HERE\n",
        "\n",
        "# plot loss across epoch\n",
        "# ENTER CODE HERE\n",
        "\n",
        "# report model accuracy on T2P test set\n",
        "# ENTER CODE HERE"
      ],
      "metadata": {
        "id": "j9NsbYcNvVRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**iv) Make a bar plot showing the test accuracy across all four models that you built. (10 points)**\n",
        "\n",
        "You can use https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.bar.html \n",
        "\n",
        "1.   logistic regression using alveolar macrophage \n",
        "2.   logistic regression using type II pneumocytes \n",
        "3.   neural network using alveolar macrophage\n",
        "4.   neural network using type II pneumocytes"
      ],
      "metadata": {
        "id": "RTqgz0nNvVlU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# make bar plot\n",
        "# ENTER CODE HERE\n"
      ],
      "metadata": {
        "id": "qjnqOknNvV9z"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}